{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  '/kaggle/input/requirements/requirements/torch_1.5/torch-1.5.0cu101-cp37-cp37m-linux_x86_64.whl'\n",
    "!pip install  '/kaggle/input/requirements/requirements/torch_1.5/torchvision-0.6.0cu101-cp37-cp37m-linux_x86_64.whl'\n",
    "!pip install  '/kaggle/input/requirements/requirements/torch_1.5/yacs-0.1.7-py3-none-any.whl'\n",
    "!pip install  '/kaggle/input/requirements/requirements/torch_1.5/fvcore-0.1.1.post200513-py3-none-any.whl'\n",
    "!pip install  '/kaggle/input/requirements/requirements/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl'\n",
    "!pip install  '/kaggle/input/requirements/requirements/detectron2/detectron2-0.1.3cu101-cp37-cp37m-linux_x86_64.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../input/globalwheat/wheat/models/yolov5/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/globalwheat/wheat/imports/transforms.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps '../input/weightedboxesfusion/' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../input/weightedboxesfusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import PIL\n",
    "import glob\n",
    "import math  \n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import detectron2\n",
    "import itertools\n",
    "import transforms as T\n",
    "import argparse\n",
    "import shutil as sh\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from tqdm.auto import tqdm\n",
    "from utils.datasets import *\n",
    "from ensemble_boxes import *\n",
    "from models.experimental import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.data import DatasetMapper\n",
    "setup_logger()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 1\n",
    "DEST_IMG_SIZE = 512\n",
    "SRC_IMG_SIZE = 1024\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "RESNET_PATH1f = \"/kaggle/input/faster-rcnn/faster/resnet_rcnn/resnet_rcnn_1f.pt\"\n",
    "RESNET_PATH2f = \"/kaggle/input/globalwheat/wheat/models/resnet_rcnn/resnet_rcnn_2f.pt\"\n",
    "RESNET_PATH3f = \"/kaggle/input/globalwheat/wheat/models/resnet_rcnn/resnet_rcnn_3f.pt\"\n",
    "RESNET_PATH4f = \"/kaggle/input/globalwheat/wheat/models/resnet_rcnn/resnet_rcnn_4f.pt\"\n",
    "RESNET_PATH5f = \"/kaggle/input/globalwheat/wheat/models/resnet_rcnn/resnet_rcnn_5f.pt\"\n",
    "\n",
    "VGG_PATH1f = \"/kaggle/input/globalwheat/wheat/models/vgg_rcnn/vgg_rcnn_1f.pt\"\n",
    "VGG_PATH2f = \"/kaggle/input/globalwheat/wheat/models/vgg_rcnn/vgg_rcnn_2f.pt\"\n",
    "VGG_PATH3f = \"/kaggle/input/globalwheat/wheat/models/vgg_rcnn/vgg_rcnn_3f.pt\"\n",
    "VGG_PATH4f = \"/kaggle/input/globalwheat/wheat/models/vgg_rcnn/vgg_rcnn_4f.pt\"\n",
    "VGG_PATH5f = \"/kaggle/input/globalwheat/wheat/models/vgg_rcnn/vgg_rcnn_5f.pt\"\n",
    "\n",
    "DETECTRON_CFG_PATH_1f = \"/kaggle/input/globalwheat/wheat/models/detectrons/detectron2_1f\"\n",
    "DETECTRON_CFG_PATH_2f = \"/kaggle/input/globalwheat/wheat/models/detectrons/detectron2_2f\"\n",
    "DETECTRON_CFG_PATH_3f = \"/kaggle/input/globalwheat/wheat/models/detectrons/detectron2_3f\"\n",
    "DETECTRON_CFG_PATH_4f = \"/kaggle/input/globalwheat/wheat/models/detectrons/detectron2_4f\"\n",
    "DETECTRON_CFG_PATH_5f = \"/kaggle/input/globalwheat/wheat/models/detectrons/detectron2_5f\"\n",
    "\n",
    "YOLO_PATH = \"../input/globalwheat/wheat/models/yolov5/weights/best_yolov5x_wheat.pt\"\n",
    "\n",
    "F_MEASURES_PATH_FR = \"/kaggle/input/f-measures/f_measures_fr.txt\"\n",
    "F_MEASURES_PATH_D2 = \"/kaggle/input/f-measures/f_measures_d2.txt\"\n",
    "\n",
    "WHEAT_PATH = \"/kaggle/input/global-wheat-detection\"\n",
    "DARKNET_DS_PATH = \"/kaggle/input/globalwheat/wheat/yolov5x-wheat-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, root_path, dest_type, transform=None, process=\"train\", divide=False, k=5, k_pos=0, csv=None, csv_coco_format=True, preprocessing=False, return_tensor=True):\n",
    "        # root_path is the root path of the wheat dataset.\n",
    "        # dest_type is the requested type for images in the dataset ('bgr', 'gray', 'rgb').\n",
    "        # transform is the augmentations to pass images through.\n",
    "        # divide determines whether to divide the dataset into train/validation or not.\n",
    "        # W×™hen process = \"train\"/\"val\" and k, k_pos --> if divide is TRUE --> determine how to divide the paths\n",
    "        # to train/validation path lists, by k-fold cross validation. k_pos is the position in the list of paths.\n",
    "        # to extract images from, to create the validation paths. There are k different k_pos within the list of paths.\n",
    "        # process = \"test\" causes get_item() method to return only the current image and its id.\n",
    "        # csv is a path to a csv file, written in the form (different from Kaggle's format): 'image_id', 'x', 'y', 'w', 'h'.\n",
    "        # If csv is None then the WheatDataset reads Kaggle's csv according to the root_path.\n",
    "        # preprocessing togather with a \"train\" process determines whether it is necessary to return image_id from get_item() method or not.\n",
    "        # csv_coco_format and the given csv argument determine together if a format conversion is needed for using the data.\n",
    "        # return_tensor determines if the returning arrays of the image and its targets should be a tensor or not \n",
    "        \n",
    "        assert root_path is not dest_type, \"Paths need to be unique!\"\n",
    "        assert dest_type in ['rgb', 'bgr', 'gray'], \"Invalid types!\"\n",
    "        assert process in ['train', 'val', 'test'], \"Invalid process!\"\n",
    "        assert k_pos < k, \"Incompatible ratio between k and k_pos!\"\n",
    "\n",
    "        super(WheatDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.process = process\n",
    "        self.dest_type = dest_type\n",
    "        self.preprocessing = preprocessing\n",
    "        self.is_pascal = not csv_coco_format\n",
    "        self.return_tensor = return_tensor\n",
    "\n",
    "        self.csv = csv\n",
    "        if self.process != \"test\":\n",
    "            self.paths = glob.glob(root_path + \"/train/*.jpg\")\n",
    "\n",
    "            if csv is None:\n",
    "                self.csv = WheatDataset.adjust_csv(root_path + '/train.csv')\n",
    "            if divide:\n",
    "                if self.process == \"train\":\n",
    "                    before_pos = self.paths[: len(self.paths) * k_pos // k]\n",
    "                    after_pos = self.paths[len(self.paths) * (k_pos + 1) // k : len(self.paths)]\n",
    "                    self.paths = before_pos + after_pos\n",
    "                elif self.process == \"val\":\n",
    "                    self.paths = self.paths[len(self.paths) * k_pos // k : len(self.paths) * (k_pos + 1) // k]\n",
    "        elif self.process == \"test\":\n",
    "            self.paths = glob.glob(root_path + \"/test/*.jpg\")\n",
    "\n",
    "        if not self.preprocessing:      \n",
    "            random.shuffle(self.paths) # Shuffle the paths\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def adjust_csv(path):\n",
    "        # Given a path for the wheat csv file,\n",
    "        # Returns the deserved csv to extract data from\n",
    "\n",
    "        # Reads csv file from path\n",
    "        new_csv = pd.read_csv(path)\n",
    "\n",
    "        # Gets an array of arrays of all bboxes in the form [[x], [y], [w], [h]]\n",
    "        bboxes = np.stack(new_csv['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep = ',')))\n",
    "\n",
    "        # Saves the values in the respective column\n",
    "        for i, col in enumerate(['x', 'y', 'w', 'h']):\n",
    "            new_csv[col] = bboxes[:, i]\n",
    "\n",
    "        new_csv.drop(columns=['bbox'], inplace=True)\n",
    "        new_csv.drop(columns=['source'], inplace=True)\n",
    "        new_csv.drop(columns=['width'], inplace=True)\n",
    "        new_csv.drop(columns=['height'], inplace=True)\n",
    "\n",
    "        return new_csv\n",
    "\n",
    "\n",
    "    def get_compatible_targets(self, img_id, idx):\n",
    "        # Gets compatible targets by img_id as a key\n",
    "\n",
    "        if self.process == \"train\" or self.process == \"val\":\n",
    "            boxes, labels, area, iscrowd = list(), list(), 0, list()\n",
    "            temp_csv = self.csv.loc[self.csv['image_id'] == img_id]\n",
    "            boxes = temp_csv[['x', 'y', 'w', 'h']].values\n",
    "\n",
    "            if len(boxes) > 0 and not self.is_pascal:\n",
    "                # Converting from Coco to Pascal_voc format\n",
    "                boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "                boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "                area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones(boxes.shape[0], dtype=torch.int64) # One class (Wheat)\n",
    "            iscrowd = torch.zeros(boxes.shape[0], dtype=torch.int64) # suppose all instances are not crowd\n",
    "\n",
    "            target = dict()\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "            return target\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        x = cv2.imread(p)\n",
    "        if self.dest_type is 'rgb':\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "        elif self.dest_type is 'gray':\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        img_str_id = p[p.rfind('/') + 1 : p.rfind('.')]\n",
    "\n",
    "        if self.process == \"train\" or self.process == \"val\":\n",
    "            y = self.get_compatible_targets(img_str_id, idx)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(**{\n",
    "                    'image': x,\n",
    "                    'bboxes': y['boxes'],\n",
    "                    'labels': y['labels'],\n",
    "                    'area': y['area'],\n",
    "                    'crowd': y['iscrowd'],\n",
    "                    'id': y['image_id']\n",
    "                })\n",
    "                \n",
    "                x = sample['image']\n",
    "                y['boxes'] = torch.as_tensor(sample['bboxes'], dtype=torch.float32)\n",
    "                y['area'] = torch.as_tensor(sample['area'], dtype=torch.float32)\n",
    "                y['iscrowd'] = torch.as_tensor(sample['crowd'], dtype=torch.int64)\n",
    "                y['labels'] = torch.as_tensor(sample['labels'], dtype=torch.int64)\n",
    "                y['image_id'] = torch.as_tensor(sample['id'], dtype=torch.int64)\n",
    "\n",
    "            if self.return_tensor:\n",
    "                x, y = T.ToTensor()(x, y)\n",
    "\n",
    "            if self.preprocessing:\n",
    "                return x, y, img_str_id\n",
    "            return x, y\n",
    "        elif self.process == \"test\":\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(**{\n",
    "                    'image': x\n",
    "                })\n",
    "                x = sample['image']\n",
    "\n",
    "            if self.return_tensor:\n",
    "                to_tensor = A.Compose([ToTensorV2(p=1.0, always_apply=True)])(**{ 'image': x })            \n",
    "                x = sample['image']\n",
    "\n",
    "            return x, img_str_id # No label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wheat validation dataloader creation\n",
    "wheat_testset = WheatDataset(WHEAT_PATH, dest_type='rgb', transform=None, process=\"test\", return_tensor=False)\n",
    "\n",
    "# Wheat validation dataloader creation\n",
    "wheat_testloader = DataLoader(wheat_testset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_rcnn_1f = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "resnet_rcnn_1f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "resnet_rcnn_1f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "resnet_rcnn_1f.to(device)\n",
    "\n",
    "resnet_rcnn_checkpoint = torch.load(RESNET_PATH1f, map_location=device)\n",
    "resnet_rcnn_1f.load_state_dict(resnet_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_rcnn_2f = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "resnet_rcnn_2f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "resnet_rcnn_2f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "resnet_rcnn_2f.to(device)\n",
    "\n",
    "resnet_rcnn_checkpoint = torch.load(RESNET_PATH2f, map_location=device)\n",
    "resnet_rcnn_2f.load_state_dict(resnet_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_rcnn_3f = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "resnet_rcnn_3f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "resnet_rcnn_3f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "resnet_rcnn_3f.to(device)\n",
    "\n",
    "resnet_rcnn_checkpoint = torch.load(RESNET_PATH3f, map_location=device)\n",
    "resnet_rcnn_3f.load_state_dict(resnet_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_rcnn_4f = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "resnet_rcnn_4f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "resnet_rcnn_4f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "resnet_rcnn_4f.to(device)\n",
    "\n",
    "resnet_rcnn_checkpoint = torch.load(RESNET_PATH4f, map_location=device)\n",
    "resnet_rcnn_4f.load_state_dict(resnet_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_rcnn_5f = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "\n",
    "resnet_rcnn_5f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "resnet_rcnn_5f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "resnet_rcnn_5f.to(device)\n",
    "\n",
    "resnet_rcnn_checkpoint = torch.load(RESNET_PATH5f, map_location=device)\n",
    "resnet_rcnn_5f.load_state_dict(resnet_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512)), aspect_ratios=((0.5, 1.0, 2.0)))\n",
    "\n",
    "class BoxHead(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BoxHead, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(*list(model.classifier._modules.values())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False)\n",
    "\n",
    "vgg_backbone = vgg.features[:-1]\n",
    "vgg_backbone.out_channels = 512\n",
    "\n",
    "vgg_box_head = BoxHead(vgg)\n",
    "\n",
    "vgg_rcnn_1f = torchvision.models.detection.faster_rcnn.FasterRCNN(\n",
    "    vgg_backbone,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    box_roi_pool = roi_pooler,\n",
    "    box_head = vgg_box_head,\n",
    "    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, num_classes=NUM_CLASSES))\n",
    "\n",
    "# Changing out_features of model according to our number of classes\n",
    "vgg_rcnn_1f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "vgg_rcnn_1f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "vgg_rcnn_1f.to(device)\n",
    "\n",
    "vgg_rcnn_checkpoint = torch.load(VGG_PATH1f, map_location=(device))\n",
    "vgg_rcnn_1f.load_state_dict(vgg_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False)\n",
    "\n",
    "vgg_backbone = vgg.features[:-1]\n",
    "vgg_backbone.out_channels = 512\n",
    "\n",
    "vgg_box_head = BoxHead(vgg)\n",
    "\n",
    "vgg_rcnn_2f = torchvision.models.detection.faster_rcnn.FasterRCNN(\n",
    "    vgg_backbone,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    box_roi_pool = roi_pooler,\n",
    "    box_head = vgg_box_head,\n",
    "    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, num_classes=NUM_CLASSES))\n",
    "\n",
    "# Changing out_features of model according to our number of classes\n",
    "vgg_rcnn_2f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "vgg_rcnn_2f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "vgg_rcnn_2f.to(device)\n",
    "\n",
    "vgg_rcnn_checkpoint = torch.load(VGG_PATH2f, map_location=(device))\n",
    "vgg_rcnn_2f.load_state_dict(vgg_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False)\n",
    "\n",
    "vgg_backbone = vgg.features[:-1]\n",
    "vgg_backbone.out_channels = 512\n",
    "\n",
    "vgg_box_head = BoxHead(vgg)\n",
    "\n",
    "vgg_rcnn_3f = torchvision.models.detection.faster_rcnn.FasterRCNN(\n",
    "    vgg_backbone,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    box_roi_pool = roi_pooler,\n",
    "    box_head = vgg_box_head,\n",
    "    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, num_classes=NUM_CLASSES))\n",
    "\n",
    "# Changing out_features of model according to our number of classes\n",
    "vgg_rcnn_3f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "vgg_rcnn_3f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "vgg_rcnn_3f.to(device)\n",
    "\n",
    "vgg_rcnn_checkpoint = torch.load(VGG_PATH3f, map_location=device)\n",
    "vgg_rcnn_3f.load_state_dict(vgg_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False)\n",
    "\n",
    "vgg_backbone = vgg.features[:-1]\n",
    "vgg_backbone.out_channels = 512\n",
    "\n",
    "vgg_box_head = BoxHead(vgg)\n",
    "\n",
    "vgg_rcnn_4f = torchvision.models.detection.faster_rcnn.FasterRCNN(\n",
    "    vgg_backbone,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    box_roi_pool = roi_pooler,\n",
    "    box_head = vgg_box_head,\n",
    "    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, num_classes=NUM_CLASSES))\n",
    "\n",
    "# Changing out_features of model according to our number of classes\n",
    "vgg_rcnn_4f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "vgg_rcnn_4f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "vgg_rcnn_4f.to(device)\n",
    "\n",
    "vgg_rcnn_checkpoint = torch.load(VGG_PATH4f, map_location=device)\n",
    "vgg_rcnn_4f.load_state_dict(vgg_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=False)\n",
    "\n",
    "vgg_backbone = vgg.features[:-1]\n",
    "vgg_backbone.out_channels = 512\n",
    "\n",
    "vgg_box_head = BoxHead(vgg)\n",
    "\n",
    "vgg_rcnn_5f = torchvision.models.detection.faster_rcnn.FasterRCNN(\n",
    "    vgg_backbone,\n",
    "    rpn_anchor_generator = anchor_generator,\n",
    "    box_roi_pool = roi_pooler,\n",
    "    box_head = vgg_box_head,\n",
    "    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, num_classes=NUM_CLASSES))\n",
    "\n",
    "# Changing out_features of model according to our number of classes\n",
    "vgg_rcnn_5f.roi_heads.box_predictor.cls_score.out_features = NUM_CLASSES\n",
    "vgg_rcnn_5f.roi_heads.box_predictor.bbox_pred.out_features = NUM_CLASSES * 4\n",
    "\n",
    "vgg_rcnn_5f.to(device)\n",
    "\n",
    "vgg_rcnn_checkpoint = torch.load(VGG_PATH5f, map_location=device)\n",
    "vgg_rcnn_5f.load_state_dict(vgg_rcnn_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnns = [resnet_rcnn_1f, resnet_rcnn_2f, resnet_rcnn_3f, resnet_rcnn_4f, resnet_rcnn_5f,\n",
    "         vgg_rcnn_1f, vgg_rcnn_2f, vgg_rcnn_3f, vgg_rcnn_4f, vgg_rcnn_5f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wheat_dicts_for_detectron(dataset_path, k_pos, k=5, dest_type=\"rgb\", transforms=None, process=\"train\"):\n",
    "    \n",
    "    ds = WheatDataset(dataset_path, dest_type, transform=transforms, divide=True, preprocessing=True, k=k, k_pos=k_pos, process=process)\n",
    "    dl = DataLoader(ds, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    dataset_dicts = list()\n",
    "    for images, targets, image_ids in tqdm(dl): # batch size = 1\n",
    "        record = dict()\n",
    "        record[\"file_name\"] = f'{dataset_path}/{process}/{image_ids[0]}.jpg'\n",
    "        record[\"image_id\"] = targets[0]['image_id']\n",
    "        record[\"height\"] = images[0].shape[1]\n",
    "        record[\"width\"] = images[0].shape[2]\n",
    "\n",
    "        annotations = list()\n",
    "        for box in targets[0]['boxes']:\n",
    "            xmin = box[0].item()\n",
    "            ymin = box[1].item()\n",
    "            xmax = box[2].item()\n",
    "            ymax = box[3].item()\n",
    "\n",
    "            poly = [\n",
    "                (xmin, ymin), (xmax, ymin), \n",
    "                (xmax, ymax), (xmin, ymax)\n",
    "            ]\n",
    "            poly = list(itertools.chain.from_iterable(poly))\n",
    "\n",
    "            anno = {\n",
    "              \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "              \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "              \"segmentation\": [poly],\n",
    "              \"category_id\": 0, # Wheat\n",
    "              \"iscrowd\": 0\n",
    "            }\n",
    "            annotations.append(anno)\n",
    "\n",
    "        record[\"annotations\"] = annotations\n",
    "        dataset_dicts.append(record)\n",
    "        \n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm([\"train\", \"val\"]):\n",
    "    DatasetCatalog.register(\"wheat1f_\" + p, lambda p=p: create_wheat_dicts_for_detectron(WHEAT_PATH, k_pos=0, k=5, process=p))\n",
    "    MetadataCatalog.get(\"wheat1f_\" + p).set(thing_classes=['Wheat'])\n",
    "\n",
    "statement_metadata = MetadataCatalog.get(\"wheat1f_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm([\"train\", \"val\"]):\n",
    "    DatasetCatalog.register(\"wheat2f_\" + p, lambda p=p: create_wheat_dicts_for_detectron(WHEAT_PATH, k=5, k_pos=1, process=p))\n",
    "    MetadataCatalog.get(\"wheat2f_\" + p).set(thing_classes=['Wheat'])\n",
    "\n",
    "statement_metadata = MetadataCatalog.get(\"wheat2f_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm([\"train\", \"val\"]):\n",
    "    DatasetCatalog.register(\"wheat3f_\" + p, lambda p=p: create_wheat_dicts_for_detectron(WHEAT_PATH, k=5, k_pos=2, process=p))\n",
    "    MetadataCatalog.get(\"wheat3f_\" + p).set(thing_classes=['Wheat'])\n",
    "\n",
    "statement_metadata = MetadataCatalog.get(\"wheat3f_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm([\"train\", \"val\"]):\n",
    "    DatasetCatalog.register(\"wheat4f_\" + p, lambda p=p: create_wheat_dicts_for_detectron(WHEAT_PATH, k=5, k_pos=3, process=p))\n",
    "    MetadataCatalog.get(\"wheat4f_\" + p).set(thing_classes=['Wheat'])\n",
    "\n",
    "statement_metadata = MetadataCatalog.get(\"wheat4f_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm([\"train\", \"val\"]):\n",
    "    DatasetCatalog.register(\"wheat5f_\" + p, lambda p=p: create_wheat_dicts_for_detectron(WHEAT_PATH, k=5, k_pos=4, process=p))\n",
    "    MetadataCatalog.get(\"wheat5f_\" + p).set(thing_classes=['Wheat'])\n",
    "\n",
    "statement_metadata = MetadataCatalog.get(\"wheat5f_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_1f = get_cfg()\n",
    "\n",
    "# Gets the pretrained RetinaNet from the given path\n",
    "cfg_1f.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg_1f.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Sets number of classes to 1 (Wheat)\n",
    "cfg_1f.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "\n",
    "# Allows images without bounding boxes\n",
    "cfg_1f.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "# Sets the output directory of the current Detectron2 model\n",
    "cfg_1f.OUTPUT_DIR = DETECTRON_CFG_PATH_1f\n",
    "\n",
    "# Loading Detectron2 model\n",
    "cfg_1f.MODEL.WEIGHTS = os.path.join(cfg_1f.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# Detectron2 validation dataloader\n",
    "val_loader_1f = build_detection_test_loader(cfg_1f, \"wheat1f_val\")\n",
    "\n",
    "predictor_1f = DefaultPredictor(cfg_1f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_2f = get_cfg()\n",
    "\n",
    "# Gets the pretrained RetinaNet from the given path\n",
    "cfg_2f.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg_2f.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Sets number of classes to 1 (Wheat)\n",
    "cfg_2f.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "\n",
    "# Allows images without bounding boxes\n",
    "cfg_2f.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "# Sets the output directory of the current Detectron2 model\n",
    "cfg_2f.OUTPUT_DIR = DETECTRON_CFG_PATH_2f\n",
    "\n",
    "# Loading Detectron2 model\n",
    "cfg_2f.MODEL.WEIGHTS = os.path.join(cfg_2f.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# Detectron2 validation dataloader\n",
    "val_loader_2f = build_detection_test_loader(cfg_2f, \"wheat2f_val\")\n",
    "\n",
    "predictor_2f = DefaultPredictor(cfg_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_3f = get_cfg()\n",
    "\n",
    "# Gets the pretrained RetinaNet from the given path\n",
    "cfg_3f.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg_3f.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Sets number of classes to 1 (Wheat)\n",
    "cfg_3f.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "\n",
    "# Allows images without bounding boxes\n",
    "cfg_3f.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "# Sets the output directory of the current Detectron2 model\n",
    "cfg_3f.OUTPUT_DIR = DETECTRON_CFG_PATH_3f\n",
    "\n",
    "# Loading Detectron2 model\n",
    "cfg_3f.MODEL.WEIGHTS = os.path.join(cfg_3f.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# Detectron2 validation dataloader\n",
    "val_loader_3f = build_detection_test_loader(cfg_3f, \"wheat3f_val\")\n",
    "\n",
    "predictor_3f = DefaultPredictor(cfg_3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_4f = get_cfg()\n",
    "\n",
    "# Gets the pretrained RetinaNet from the given path\n",
    "cfg_4f.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg_4f.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Sets number of classes to 1 (Wheat)\n",
    "cfg_4f.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "\n",
    "# Allows images without bounding boxes\n",
    "cfg_4f.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "# Sets the output directory of the current Detectron2 model\n",
    "cfg_4f.OUTPUT_DIR = DETECTRON_CFG_PATH_4f\n",
    "\n",
    "# Loading Detectron2 model\n",
    "cfg_4f.MODEL.WEIGHTS = os.path.join(cfg_4f.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# Detectron2 validation dataloader\n",
    "val_loader_4f = build_detection_test_loader(cfg_4f, \"wheat4f_val\")\n",
    "\n",
    "predictor_4f = DefaultPredictor(cfg_4f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_5f = get_cfg()\n",
    "\n",
    "# Gets the pretrained RetinaNet from the given path\n",
    "cfg_5f.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
    "cfg_5f.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Sets number of classes to 1 (Wheat)\n",
    "cfg_5f.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "\n",
    "# Allows images without bounding boxes\n",
    "cfg_5f.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "# Sets the output directory of the current Detectron2 model\n",
    "cfg_5f.OUTPUT_DIR = DETECTRON_CFG_PATH_5f\n",
    "\n",
    "# Loading Detectron2 model\n",
    "cfg_5f.MODEL.WEIGHTS = os.path.join(cfg_5f.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# Detectron2 validation dataloader\n",
    "val_loader_5f = build_detection_test_loader(cfg_5f, \"wheat5f_val\")\n",
    "\n",
    "predictor_5f = DefaultPredictor(cfg_5f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectrons = [predictor_1f, predictor_2f, predictor_3f, predictor_4f, predictor_5f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "del vgg_rcnn_checkpoint\n",
    "del resnet_rcnn_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf(boxes, scores, image_size=1023, iou_thr=0.5, skip_box_thr=0.7, weights=None):\n",
    "    #boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n",
    "    #scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n",
    "    labels = [np.zeros(score.shape[0]) for score in scores]\n",
    "    boxes = [box/(image_size) for box in boxes]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    #boxes, scores, labels = nms(boxes, scores, labels, weights=[1,1,1,1,1], iou_thr=0.5)\n",
    "    boxes = boxes*(image_size)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TTAImage(image, index):\n",
    "    image1 = image.copy()\n",
    "    if index==0: \n",
    "        rotated_image = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n",
    "        return rotated_image\n",
    "    elif index==1:\n",
    "        rotated_image2 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n",
    "        rotated_image2 = cv2.rotate(rotated_image2, cv2.ROTATE_90_CLOCKWISE)\n",
    "        return rotated_image2\n",
    "    elif index==2:\n",
    "        rotated_image3 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n",
    "        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n",
    "        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n",
    "        return rotated_image3\n",
    "    elif index == 3:\n",
    "        return image1\n",
    "    \n",
    "    \n",
    "def rotBoxes90(boxes, im_w, im_h):\n",
    "    ret_boxes =[]\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        x1, y1, x2, y2 = x1-im_w//2, im_h//2 - y1, x2-im_w//2, im_h//2 - y2\n",
    "        x1, y1, x2, y2 = y1, -x1, y2, -x2\n",
    "        x1, y1, x2, y2 = int(x1+im_w//2), int(im_h//2 - y1), int(x2+im_w//2), int(im_h//2 - y2)\n",
    "        x1a, y1a, x2a, y2a = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\n",
    "        ret_boxes.append([x1a, y1a, x2a, y2a])\n",
    "    return np.array(ret_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect1Image_aug(im0, imgsz, model, device, conf_thres, iou_thres):\n",
    "    img = letterbox(im0, new_shape=imgsz)[0]\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img =  img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0   \n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        # save_path = 'draw/' + image_id + '.jpg'\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in det:\n",
    "                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n",
    "                scores.append(conf)\n",
    "\n",
    "    return np.array(boxes), np.array(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading YOLOv5 model, trained on the the original data and additionally on the pseudo test labels\n",
    "yolov5 = torch.load(\"/kaggle/input/globalwheat/wheat/models/yolov5/weights/best_yolov5x_wheat.pt\", map_location=device)['model'].float()\n",
    "yolov5.to(device).eval();\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(img, boxes, text=None, text_size=None, thickness=3):\n",
    "    # Draws all the given bounding boxes (boxes) within the given image (img), using a thickness value,\n",
    "    # a text list corresponding with each bounding box, and its size\n",
    "    \n",
    "    new_img = img.copy()\n",
    "    for b, t in zip(boxes, text):\n",
    "        start_point = (int(b[0].item()), int(b[1].item()))\n",
    "        end_point = (int(b[2].item()), int(b[3].item()))\n",
    "        color = (0, 255, 255) \n",
    "        new_img = cv2.rectangle(new_img, start_point, end_point, color, thickness)\n",
    "        if text is not None and text_size is not None:\n",
    "            cv2.putText(new_img, t, (int(b[0].item()), int(b[1].item()) - 2), cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 255, 255), 4)\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ensemble(faster_rcnns, detectrons2, yolov5, testloader, faster_rcnn_weights_path, detectron2_weights_path, score_thresh=[0.35, 0.35, 0.35, 0.35], iou_thresh=[0.4, 0.4, 0.4, 0.4]):\n",
    "    # Given a detection ensemble of models (faster_rcnns, detectron2 models and a yolo), a test dataloader, a path to texts file which include f-measures of each model,\n",
    "    # and iou/score threshold lists, for each model type: typei --> thresh(i). The thresholds of the whole ensemble is in the last indexes of these lists.\n",
    "    # Returns a dictionary including the results of the given ensemble according to the submission format of the wheat competition\n",
    "\n",
    "    results, fr_f_measures, d2_f_measures = list(), list(), list()\n",
    "\n",
    "    # Reads each f-measure corresponding with each model (five versions (folds) of resnet, \n",
    "    # vgg Faster RCNNs or detectrons version 2), by the given path of a text file.\n",
    "    # These F-Measures are the weights of the models in the ensemble\n",
    "    with open(faster_rcnn_weights_path, 'r') as eval_file:\n",
    "        line = eval_file.readline()\n",
    "        while line:\n",
    "            fr_f_measures.append(float(line))\n",
    "            line = eval_file.readline()\n",
    "    with open(detectron2_weights_path, 'r') as eval_file:\n",
    "        line = eval_file.readline()\n",
    "        while line:\n",
    "            d2_f_measures.append(float(line))\n",
    "            line = eval_file.readline()\n",
    "    \n",
    "    # Collects predictions of each model\n",
    "    for images, image_ids in tqdm(testloader): # batch size = 1\n",
    "        # img --> for the predictions of YOLOv5 and Detectron2 models\n",
    "        img = images[0]\n",
    "        \n",
    "        # resized_img --> for the predictions of Faster RCNN models\n",
    "        # Resizing and normalizing the image to match faster rcnn test predictions (trained on image size of 512 X 512 and range (0-1))\n",
    "        resized_img = cv2.resize(img, (DEST_IMG_SIZE, DEST_IMG_SIZE), cv2.INTER_LINEAR) / DEST_IMG_SIZE # Resizing image to original size\n",
    "        \n",
    "        all_fr_boxes, all_fr_scores, all_fr_labels = list(), list(), list()\n",
    "        all_d2_boxes, all_d2_scores, all_d2_labels = list(), list(), list()\n",
    "        all_y_boxes, all_y_scores, all_y_labels = list(), list(), list()\n",
    "\n",
    "        for fr in faster_rcnns:\n",
    "            fr.eval()\n",
    "\n",
    "            tta_fr_boxes, tta_fr_scores, tta_fr_labels = list(), list(), list()\n",
    "\n",
    "            for i in range(4):\n",
    "                resized_img_ = torch.from_numpy(TTAImage(resized_img, i)).permute(2, 0, 1).float().to(device) # (3, 512, 512), tensor, 0-1\n",
    "                \n",
    "                # Current Faster RCNN prediction using TTA\n",
    "                fr_prediction = fr([resized_img_])[0] # batch size = 1\n",
    "                fr_boxes = fr_prediction['boxes'].data.cpu().numpy()\n",
    "                fr_scores = fr_prediction['scores'].data.cpu().numpy().tolist()\n",
    "\n",
    "                for _ in range(3-i): # Reordering the boxes\n",
    "                    fr_boxes = rotBoxes90(fr_boxes, resized_img_.shape[1], resized_img_.shape[1])\n",
    "\n",
    "                tta_fr_boxes.append((fr_boxes / resized_img_.shape[1]).tolist())\n",
    "                tta_fr_scores.append(fr_scores)\n",
    "                tta_fr_labels.append([1] * len(fr_boxes))\n",
    "                        \n",
    "            tta_fr_boxes, tta_fr_scores, tta_fr_labels = weighted_boxes_fusion(tta_fr_boxes, tta_fr_scores, tta_fr_labels, weights=None, conf_type='avg', iou_thr=0.9, skip_box_thr=0.15)\n",
    "            all_fr_boxes.append(tta_fr_boxes)    \n",
    "            all_fr_scores.append(tta_fr_scores)\n",
    "            all_fr_labels.append(tta_fr_labels) # Wheat --> 1\n",
    "            \n",
    "        for d2 in detectrons2:         \n",
    "            tta_d2_boxes, tta_d2_scores, tta_d2_labels = list(), list(), list()\n",
    "            for i in range(4):\n",
    "                img_ = TTAImage(img, i) # (1024, 1024, 3), numpy, 0-255\n",
    "                        \n",
    "                # Current Detectron2 prediction using TTA\n",
    "                d2_prediction = d2(img_) # batch size = 1\n",
    "                d2_boxes = d2_prediction['instances'].get_fields()['pred_boxes'].tensor.data.cpu().numpy()\n",
    "                d2_scores = d2_prediction['instances'].get_fields()['scores'].data.cpu().numpy().tolist()\n",
    "\n",
    "                for _ in range(3-i): # Reordering the boxes\n",
    "                    d2_boxes = rotBoxes90(d2_boxes, img_.shape[1], img_.shape[1])\n",
    "                \n",
    "                tta_d2_boxes.append((d2_boxes / img_.shape[1]).tolist())\n",
    "                tta_d2_scores.append(d2_scores)\n",
    "                tta_d2_labels.append([1] * len(d2_boxes))\n",
    "                        \n",
    "            tta_d2_boxes, tta_d2_scores, tta_d2_labels = weighted_boxes_fusion(tta_d2_boxes, tta_d2_scores, tta_d2_labels, weights=None, conf_type='avg', iou_thr=0.9, skip_box_thr=0.15)\n",
    "            all_d2_boxes.append(tta_d2_boxes)    \n",
    "            all_d2_scores.append(tta_d2_scores)\n",
    "            all_d2_labels.append(tta_d2_labels) # Wheat --> 1\n",
    "        \n",
    "        for i in range(4):\n",
    "            img_ = TTAImage(img, i) # (1024, 1024, 3), numpy, 0-255\n",
    "            \n",
    "            # Current Detectron2 prediction using TTA\n",
    "            y_boxes, y_scores = detect1Image_aug(img_, img_.shape[1], yolov5, device, score_thresh[2], iou_thresh[2])\n",
    "            y_scores = [s.cpu().item() for s in y_scores]\n",
    "\n",
    "            for _ in range(3-i): # Reordering the boxes\n",
    "                y_boxes = rotBoxes90(y_boxes, img_.shape[1], img_.shape[1])\n",
    "\n",
    "            y_boxes = [b / img_.shape[1] for b in y_boxes] # Normalizing the boxes\n",
    "            all_y_boxes.append(y_boxes)    \n",
    "            all_y_scores.append(y_scores)\n",
    "            all_y_labels.append([1] * len(y_boxes)) # Wheat --> 1\n",
    "                            \n",
    "        # Gets predictions of the main three models (5-fold Faster RCNN and Detectron2 models, and a YOLOv5 predictions, using TTA augmentations)\n",
    "        # Gets current batch average predictions by the faster_rcnn models\n",
    "        fr_boxes, fr_scores, fr_labels = weighted_boxes_fusion(all_fr_boxes, all_fr_scores, all_fr_labels, weights=fr_f_measures, conf_type='avg', iou_thr=iou_thresh[0], skip_box_thr=score_thresh[0])\n",
    "\n",
    "        # Gets current batch average predictions by the detectron2 predictors\n",
    "        d2_boxes, d2_scores, d2_labels = weighted_boxes_fusion(all_d2_boxes, all_d2_scores, all_d2_labels, weights=d2_f_measures, conf_type='avg', iou_thr=iou_thresh[1], skip_box_thr=score_thresh[1])\n",
    "\n",
    "        # Gets current batch average predictions by the YOLOv5 single model\n",
    "        y_boxes, y_scores, y_labels = weighted_boxes_fusion([y_boxes], [y_scores], [[1] * len(y_boxes)], weights=None, conf_type='avg', iou_thr=0.9, skip_box_thr=0.15)\n",
    "         \n",
    "        # Mixing all of the models\n",
    "        boxes = [y_boxes, fr_boxes, d2_boxes]\n",
    "        scores = [y_scores, fr_scores, d2_scores]\n",
    "        labels = [y_labels, fr_labels, d2_labels]\n",
    "\n",
    "        # Gets the prediction of the ensemble, containing Faster RCNN, Detectron2 and YOLOv5 models\n",
    "        boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=[1, 0, 0], conf_type='max', iou_thr=iou_thresh[3], skip_box_thr=score_thresh[3])\n",
    "\n",
    "        resized_boxes = np.array([])\n",
    "        if len(boxes) > 0:\n",
    "            resized_boxes = np.asarray(np.asarray(boxes, dtype=np.float64) * img.shape[1], dtype=np.int32).clip(min=0, max=1023) # Resizing boxes to original size\n",
    "            scores_str = ['{0:.2f}'.format(s) for s in scores]\n",
    "            img = draw_bboxes(img, resized_boxes, scores_str, 1.5, thickness=5) # Draws bboxes within the current image\n",
    "                \n",
    "            # Converting from Pascal_voc to Coco format\n",
    "            resized_boxes[:, 2] = resized_boxes[:, 2] - resized_boxes[:, 0]\n",
    "            resized_boxes[:, 3] = resized_boxes[:, 3] - resized_boxes[:, 1]\n",
    "\n",
    "        result = {\n",
    "            'image_id': image_ids[0],\n",
    "            'PredictionString': format_prediction_string(resized_boxes, scores),\n",
    "            'image': img\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_ensemble(rcnns, detectrons, yolov5, wheat_testloader, F_MEASURES_PATH_FR, F_MEASURES_PATH_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(results[3]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
